{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978d5326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers, Input, Model, regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "# 1. --- Load Data ---\n",
    "df = pd.read_csv('all_data_labeled_final6.csv')\n",
    "feature_cols = [\n",
    "    'attention', 'meditation', 'delta', 'theta',\n",
    "    'lowAlpha', 'highAlpha', 'lowBeta', 'highBeta',\n",
    "    'lowGamma', 'highGamma', 'blinkStrength', 'time'\n",
    "]\n",
    "X_raw = df[feature_cols].values\n",
    "y_raw = df['blinkType'].values\n",
    "session_ids = df['session_id'].values\n",
    "\n",
    "# 2. --- Feature Engineering ---\n",
    "WINDOW_SIZE = 20   # Should be enough to capture two blinks for a double\n",
    "STEP_SIZE = 3\n",
    "\n",
    "def last_blink_interval(window, blink_idx, time_idx, threshold=60):\n",
    "    times = window[:, time_idx]\n",
    "    blinks = window[:, blink_idx] >= threshold\n",
    "    blink_times = times[blinks]\n",
    "    if len(blink_times) > 1:\n",
    "        return blink_times[-1] - blink_times[-2]\n",
    "    return 0.0\n",
    "\n",
    "def make_window_features(X, y, session_ids, window_size=20, step_size=3):\n",
    "    windows, labels, feats = [], [], []\n",
    "    for start in range(0, len(X) - window_size + 1, step_size):\n",
    "        end = start + window_size\n",
    "        if len(np.unique(session_ids[start:end])) == 1:\n",
    "            window = X[start:end]\n",
    "            # Window's label: last label in window (can be 2 now if it's a double)\n",
    "            label = y[end-1]\n",
    "            windows.append(window)\n",
    "            labels.append(label)\n",
    "            blink_idx = feature_cols.index('blinkStrength')\n",
    "            time_idx = feature_cols.index('time')\n",
    "            blink_vals = window[:, blink_idx]\n",
    "            # Feature set\n",
    "            feats_window = [\n",
    "                np.mean(blink_vals), np.std(blink_vals), np.min(blink_vals), np.max(blink_vals),\n",
    "                blink_vals[-1], np.ptp(blink_vals),\n",
    "                np.mean(np.diff(blink_vals)), np.std(np.diff(blink_vals)),\n",
    "                np.sum(blink_vals > 60),\n",
    "                np.sum(blink_vals == 0),\n",
    "                np.min(window[:, time_idx]), np.max(window[:, time_idx]), np.ptp(window[:, time_idx])\n",
    "            ]\n",
    "            for band in ['delta','theta','lowAlpha','highAlpha','lowBeta','highBeta','lowGamma','highGamma']:\n",
    "                idx = feature_cols.index(band)\n",
    "                feats_window += [np.mean(window[:, idx]), np.std(window[:, idx])]\n",
    "            feats_window.append(last_blink_interval(window, blink_idx, time_idx))\n",
    "            feats.append(feats_window)\n",
    "    return (np.array(windows), np.array(labels), np.array(feats))\n",
    "\n",
    "X_win, y_win, X_feats = make_window_features(X_raw, y_raw, session_ids, WINDOW_SIZE, STEP_SIZE)\n",
    "\n",
    "# 3. --- Scale Features ---\n",
    "scaler_seq = StandardScaler()\n",
    "X_win_flat = X_win.reshape(-1, X_win.shape[-1])\n",
    "X_win_scaled = scaler_seq.fit_transform(X_win_flat).reshape(X_win.shape)\n",
    "joblib.dump(scaler_seq, 'scaler_seq.pkl')\n",
    "\n",
    "scaler_feats = StandardScaler()\n",
    "X_feats_scaled = scaler_feats.fit_transform(X_feats)\n",
    "joblib.dump(scaler_feats, 'scaler_feats.pkl')\n",
    "\n",
    "# 4. --- Train/Test Split ---\n",
    "X_temp, X_test, Xf_temp, Xf_test, y_temp, y_test = train_test_split(\n",
    "    X_win_scaled, X_feats_scaled, y_win, test_size=0.15, random_state=42, stratify=y_win)\n",
    "X_train, X_val, Xf_train, Xf_val, y_train, y_val = train_test_split(\n",
    "    X_temp, Xf_temp, y_temp, test_size=0.1765, random_state=42, stratify=y_temp)\n",
    "\n",
    "y_train_cat = to_categorical(y_train)\n",
    "y_val_cat = to_categorical(y_val)\n",
    "y_test_cat = to_categorical(y_test)\n",
    "\n",
    "# Save for deployment:\n",
    "with open('windowed_feature_cols.json', 'w') as f:\n",
    "    json.dump(feature_cols, f)\n",
    "with open('preprocessing_meta.json', 'w') as f:\n",
    "    json.dump({'window_size': WINDOW_SIZE, 'step_size': STEP_SIZE}, f)\n",
    "\n",
    "# 5. --- Custom Focal Loss ---\n",
    "def custom_focal_loss(alpha=None, gamma=2.):\n",
    "    if alpha is None:\n",
    "        alpha = [0.2, 1.2, 0.6]\n",
    "    alpha = tf.constant(alpha, dtype=tf.float32)\n",
    "    def loss(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1. - tf.keras.backend.epsilon())\n",
    "        ce = -y_true * tf.math.log(y_pred)\n",
    "        weight = alpha * tf.pow(1 - y_pred, gamma)\n",
    "        loss = weight * ce\n",
    "        return tf.reduce_sum(loss, axis=-1)\n",
    "    return loss\n",
    "\n",
    "# 6. --- Model: CNN + BiLSTM Hybrid ---\n",
    "seq_input = Input(shape=(WINDOW_SIZE, X_win.shape[2]), name='windowed_input')\n",
    "x = layers.Conv1D(32, kernel_size=3, activation='relu', padding='same')(seq_input)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Conv1D(32, kernel_size=3, activation='relu', padding='same')(x)\n",
    "x = layers.MaxPooling1D(2)(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Bidirectional(layers.LSTM(64, return_sequences=False, dropout=0.3))(x)\n",
    "x = layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "\n",
    "feat_input = Input(shape=(X_feats.shape[1],), name='features_input')\n",
    "y = layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(1e-4))(feat_input)\n",
    "y = layers.BatchNormalization()(y)\n",
    "y = layers.Dense(16, activation='relu')(y)\n",
    "\n",
    "merged = layers.concatenate([x, y])\n",
    "z = layers.Dense(48, activation='relu', kernel_regularizer=regularizers.l2(1e-4))(merged)\n",
    "z = layers.BatchNormalization()(z)\n",
    "z = layers.Dropout(0.2)(z)\n",
    "output = layers.Dense(3, activation='softmax')(z)\n",
    "\n",
    "model = Model(inputs=[seq_input, feat_input], outputs=output)\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=custom_focal_loss(alpha=[0.3, 1.0, 0.7], gamma=2),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "# 7. --- Train ---\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.6, min_lr=1e-5, verbose=1),\n",
    "    ModelCheckpoint('best_eeg_cnn_bilstm_focal.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "]\n",
    "history = model.fit(\n",
    "    [X_train, Xf_train], y_train_cat,\n",
    "    epochs=100,\n",
    "    batch_size=96,\n",
    "    validation_data=([X_val, Xf_val], y_val_cat),\n",
    "    callbacks=callbacks,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# 8. --- Evaluate ---\n",
    "model = tf.keras.models.load_model(\n",
    "    'best_eeg_cnn_bilstm_focal.h5',\n",
    "    custom_objects={'loss': custom_focal_loss(alpha=[0.3, 1.0, 0.7], gamma=2)}\n",
    ")\n",
    "y_test_pred_probs = model.predict([X_test, Xf_test])\n",
    "y_test_pred = np.argmax(y_test_pred_probs, axis=1)\n",
    "y_test_true = np.argmax(y_test_cat, axis=1)\n",
    "\n",
    "print(\"\\nClassification Report (Hybrid Model, Test Set):\")\n",
    "print(classification_report(y_test_true, y_test_pred, digits=4))\n",
    "\n",
    "cm = confusion_matrix(y_test_true, y_test_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(\"Confusion Matrix (Hybrid Model, Test Set)\")\n",
    "plt.show()\n",
    "\n",
    "# 9. --- Plot Learning Curves ---\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['accuracy'], label='Train Acc')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
    "plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.title('Accuracy over Epochs'); plt.legend()\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('Loss over Epochs'); plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 10. --- Show Sample Predictions ---\n",
    "for i in range(10):\n",
    "    idx = np.random.randint(0, len(y_test_true))\n",
    "    print(f\"Example {i+1}: Actual: {y_test_true[idx]}, Predicted: {y_test_pred[idx]}, Softmax: {np.round(y_test_pred_probs[idx], 2)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
